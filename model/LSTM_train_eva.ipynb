{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3736,
     "status": "ok",
     "timestamp": 1715755371093,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "EBuUd8odeVOa",
    "outputId": "e2c4aacd-1945-41bc-8751-4f41dd62161b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1715755371094,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "EVhG8QFXksCj"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/content/drive/MyDrive/FIT3162')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1715755371095,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "WZB2TcL-eR_v",
    "outputId": "ff916c5d-6c4e-42bd-fc26-658295a1cfe6"
   },
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1715755371095,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "jQa93wfnzKUr"
   },
   "outputs": [],
   "source": [
    "#Try on Proshphet model from facebook to train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1715755371096,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "A8_lh-oc38Or"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1715755371097,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "3lvPr43V_Mf5"
   },
   "outputs": [],
   "source": [
    "#pip install pyts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: mlxtend in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (0.23.1)\n",
      "Requirement already satisfied: pyts in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: filelock in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from mlxtend) (3.8.4)\n",
      "Requirement already satisfied: numba>=0.55.2 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from pyts) (0.59.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from matplotlib>=3.0.0->mlxtend) (3.1.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from numba>=0.55.2->pyts) (0.42.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch numpy pandas scikit-learn mlxtend pyts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5NVhsKinOPF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3703,
     "status": "ok",
     "timestamp": 1715755374786,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "ZfMkZimorAIb"
   },
   "outputs": [],
   "source": [
    "#Preprocess the dataset\n",
    "\n",
    "#Preprocess the data\n",
    "import torch\n",
    "import numpy as np;\n",
    "from torch.autograd import Variable\n",
    "from pyts.approximation import SymbolicAggregateApproximation\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "def normalise_sd(x):\n",
    "        #x.std() standard deviation: measure of the spread or variability of the\n",
    "        #data. The standard deviation is computed by taking the square root of the\n",
    "        #average squared deviation from the mean\n",
    "        #This factor adjusts the standard deviation to account for the fact that\n",
    "        #the sample mean is not the same as the population mean, and it is based\n",
    "        #on the degrees of freedom of the sample, which is one less than the sample size\n",
    "  #Bessel's correction, adjust the sd by mulpliying with square of (n-1)/n\n",
    "  #important when sample size is small -> provide better estimate of the population sd\n",
    "  #correction makes the sd slighly larger as compensating the fact\n",
    "  #that sample variance typically understimate of the population variance for small sample\n",
    "  return x.std() * np.sqrt((len(x) - 1)/len(x))\n",
    "\n",
    "class Data_util(object):\n",
    "   # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
    "                                                #ntp: next token prediciton\n",
    "                                                #re: rolling evaluation\n",
    "  def __init__(self, file_name, train, valid, cuda, ntp, re, normalise = 2):\n",
    "    self.cuda_is_available = cuda\n",
    "    self.re = re\n",
    "    self.ntp = ntp\n",
    "    data = open(file_name);\n",
    "    #load txt file to be file object\n",
    "    #separate dat by ,\n",
    "    self.rawdat = np.loadtxt(data,delimiter=',')\n",
    "    # perform arm on data\n",
    "    self._arm(5, 0.7, 0.7, 50)\n",
    "    self.dat = np.zeros(self.rawdat.shape)\n",
    "    self.n, self.m = self.dat.shape;\n",
    "    self.normalise = 2\n",
    "    self.scale = np.ones(self.m)\n",
    "    #_for private\n",
    "    self._normalised(normalise)\n",
    "                    #0.6 * whole dataset size -> number of rows to train\n",
    "                    #0.8 end index for the rows for valid\n",
    "    self._split(int(train * self.n), int((train+valid) * self.n), self.n)\n",
    "                  #tensor from numpy array\n",
    "    self.scale = torch.from_numpy(self.scale).float()\n",
    "                                  #reshape the dimension of the original data\n",
    "                                  #with row_count = test[1].size(0) and self.m =\n",
    "    tmp = self.test[1] * self.scale.expand(self.test[1].size(0), self.m)\n",
    "\n",
    "    if self.cuda_is_available:\n",
    "      self.scale = self.scale.cuda() # move scaling parameters into GPU from CPU\n",
    "                #wrap the tensor for pytorch to track the history of operations to th\n",
    "                #this scaling tensor for auto-diffentiation\n",
    "    self.scale = Variable(self.scale)\n",
    "    #self.rse = normal_std(tmp); calculates the root squared error (RSE) of the model output tmp\n",
    "    self.rse = normalise_sd(tmp)\n",
    "    #calculates the relative absolute error (RAE) of the model output tmp\n",
    "    self.rae = torch.mean(torch.abs(tmp - torch.mean(tmp)))\n",
    "\n",
    "  def _arm(self, n_bins, min_support, min_threshold, n_rules):\n",
    "    data = self.rawdat\n",
    "    df = pd.DataFrame(data)\n",
    "    scaler = MinMaxScaler()\n",
    "    norm_data = scaler.fit_transform(df)\n",
    "    sax = SymbolicAggregateApproximation(n_bins=n_bins, strategy='uniform')\n",
    "    sax_df = pd.DataFrame(sax.fit_transform(norm_data))\n",
    "    binary_sax_df = pd.get_dummies(sax_df)\n",
    "    frequent_itemsets = apriori(binary_sax_df, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=min_threshold)\n",
    "    rules = rules.sort_values(by=['zhangs_metric'], ascending=False).iloc[:n_rules, :]\n",
    "    unique_items = set()\n",
    "    for index, row in rules.iterrows():\n",
    "        unique_items.update([int(item[:-2]) for item in row['antecedents']])\n",
    "        unique_items.update([int(item[:-2]) for item in row['consequents']])\n",
    "    unique_items = list(unique_items)\n",
    "    df = df.iloc[:, unique_items]\n",
    "    self.rawdat = df.values\n",
    "\n",
    "  def _normalised(self, normalise):\n",
    "    #normalised by the max value of entire matrix\n",
    "    if (normalise == 0):\n",
    "      self.dat = self.rawdat\n",
    "    elif (normalise == 1):\n",
    "      self.dat = self.rawdat / np.max(self.rawdat)\n",
    "    elif (normalise == 2):\n",
    "      #normalised by the max value of each row\n",
    "      for i in range(self.m):\n",
    "        #list of scaling values for each of the data\n",
    "        self.scale[i] = np.max(np.abs(self.rawdat[:,1]))\n",
    "        #normalise the value in individual column based on the largest value in column\n",
    "        #some column largest values might be negative\n",
    "        self.dat[:,i] = self.rawdat[:,i]/np.max(np.abs(self.rawdat[:,i]))\n",
    "\n",
    "  def _split(self, train, valid, test):\n",
    "                        #leave for the self.re: rolling_evaluation (validation)\n",
    "                        #self.ntp: next few tokens prediction\n",
    "                                              #0.6\n",
    "      train_set = range(self.re+self.ntp-1, train);#save the front valus for re and ntp\n",
    "      valid_set = range(train, valid); #0.6 - 0.8\n",
    "      test_set = range(valid, self.n);#remaining for text 0.2, 0.8 - 1.0\n",
    "      #train dataset\n",
    "      self.train = self._batchify(train_set, self.ntp);\n",
    "      self.valid = self._batchify(valid_set, self.ntp);\n",
    "      self.test = self._batchify(test_set, self.ntp);\n",
    "\n",
    "  #self.train = self._batchify(train_set, self.ntp);\n",
    "  def _batchify(self, idx_set, ntp):#horizon for next prediciton\n",
    "  #number of samples in one batch\n",
    "    #index set for dataset (train_set if train_set passed)\n",
    "    #each dataset has a n\n",
    "    n = len(idx_set)  #rolling_evaluation , size of the input column\n",
    "    X = torch.zeros((n, self.re,self.m))\n",
    "    Y = torch.zeros((n,self.m))\n",
    "\n",
    "\n",
    "    for i in range(n):  #end: start of the next token prediction\n",
    "    #for each row/entry set the region for the training\n",
    "      #[train] + [rolling_evaluation]\n",
    "      end = idx_set[i] - self.ntp + 1 #save the last few token/value for next token prediciton\n",
    "      #start: start of rolling evaluation\n",
    "      start = end - self.re #save the values infront values save for self.ntp to do rolling_evalution\n",
    "      #slice the input data for training\n",
    "      #by slicing each sample/entry/row into X\n",
    "      #between start and end to be the training dataset\n",
    "\n",
    "      #create a PyTorch tensor 'X' with 5 batches, each containing a slice of 20 rows from 'data'\n",
    "      #X = torch.empty(5, 20, 10)  # Pre-initialize X with the desired shape\n",
    "      #X will contain 5 separate slices from data\n",
    "\n",
    "      #slice the many rows of data except for\n",
    "      X[i,:,:] = torch.from_numpy(self.dat[start:end, :])\n",
    "      #for batching multiple rows of data together to train from start to end\n",
    "      #his line is assigning a 2D slice of the numpy array self.dat to the i-th\n",
    "      # index in the first dimension of the tensor X\n",
    "      #start:end indicates a range of rows, so you’re selecting multiple rows and all columns within that range.\n",
    "      #This line is assigning a 1D slice (a single row) of the numpy array self.dat\n",
    "      #to the i-th index in the first dimension of the tensor Y\n",
    "      Y[i,:] = torch.from_numpy(self.dat[idx_set[i], :]);\n",
    "      #idx_set[i] is an index for a specific row, so you’re selecting just that row and all columns in it.\n",
    "      #The result is that Y[i,:] will be a 1D tensor with the same number of elements as there are columns in self.dat.\n",
    "\n",
    "\n",
    "    return [X,Y]\n",
    "\n",
    "    #train_loss = train(Data, x, y, model, criterion, optim, args.batch_size)\n",
    "    #data.get_batches(X,Y, batch_size, True):\n",
    "  def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
    "    #get_batches used in training loop to iterate over the generator to ge the batche of data\n",
    "    #for each training step\n",
    "    length = len(inputs)\n",
    "    if shuffle: #permutation to shuffle the whole row/entry, so different first value in en\n",
    "      #creates a tensor named index that contains a random permutation of\n",
    "      #integers from 0 to length-1. Then shuffle the valus according the random valus\n",
    "      index = torch.randperm(length)#randperm uses a Fisher-Yates shuffle algorithm to create a random permutation of numbers\n",
    "    else: #create a long type tensor with original arrangement\n",
    "      index = torch.LongTensor(range(length))\n",
    "    start_idx = 0\n",
    "    while (start_idx < length):\n",
    "      end_idx = min(length,start_idx + batch_size)\n",
    "      data_idx = index[start_idx:end_idx]\n",
    "      X = inputs[data_idx]\n",
    "      Y = targets[data_idx]\n",
    "      if (self.cuda_is_available):\n",
    "        X = X.cuda()\n",
    "        Y = Y.cuda()\n",
    "        #return multiple values in generator-level fashion\n",
    "        #Data.train[1] = Variable(X)\n",
    "      yield Variable(X), Variable(Y)\n",
    "      start_idx += batch_size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10318,
     "status": "ok",
     "timestamp": 1715755385071,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "8zdxRu8Jbel7",
    "outputId": "bdd24157-b2c1-47ae-9271-8aa6b90fcde8"
   },
   "outputs": [],
   "source": [
    "#python main.py --gpu 3 --horizon 24 --data data/electricity.txt --save save/elec.pt --output_fun Linear\n",
    "#args = parser.parse_args()\n",
    "#Data = Data_util(args.data, 0.6, 0.2, args.cuda, args.horizon, args.window, args.normalise);\n",
    "Data = Data_util('/Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/data/electricity.txt', 0.6, 0.2, False, 12, 24 * 7, 2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1715755385073,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "tFiGZElHdKMs",
    "outputId": "f0bd269c-0eb9-42ba-db10-437141482800"
   },
   "outputs": [],
   "source": [
    "def evaluate(data, X, Y, model, evaluateL2, evaluateL1, batch_size):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_loss_l1 = 0\n",
    "  n_samples = 0\n",
    "  predict = None\n",
    "  test = None\n",
    "\n",
    "  for X, Y in data.get_batches(X,Y, batch_size, False):\n",
    "    output = model(X)\n",
    "    #predict can be chanegd durring the for loop\n",
    "    if predict is None:\n",
    "      predict = output;\n",
    "      test = Y;\n",
    "    else:\n",
    "      predict = torch.cat((predict,output))\n",
    "      test = torch.cat((test, Y))\n",
    "\n",
    "    scale = data.scale.expand(output.size(0), data.m)\n",
    "    #computer L2 loss = mean squared error (MSE)\n",
    "    #.data[0] contain the loss value\n",
    "    #scale to bring back the value if they were normalised\n",
    "    # total_loss += evaluateL2(output * scale, Y * scale).data[0]\n",
    "    #.item() to get the loss value\n",
    "    total_loss += evaluateL2(output * scale, Y * scale).item()\n",
    "    total_loss_l1 += evaluateL1(output * scale, Y * scale).item()\n",
    "                #output.size(0) = batch_size * number of columns in the dataset\n",
    "    n_samples += (output.size(0) * data.m)\n",
    "  #rse: Root relative squared error: predictive accuracy of a model in statistics and machine learning\n",
    "  #   = mse/ population sd +-= (nomalised sample sd)\n",
    "  rse = math.sqrt(total_loss / n_samples)/data.rse\n",
    "  rae = (total_loss_l1 / n_samples)/data.rae\n",
    "\n",
    "  # converts a PyTorch tensor to a NumPy array\n",
    "  #safe way:\n",
    "  #predict = predict.detach().cpu().numpy()\n",
    "  # can lead to potential issues with the computation graph and gradient tracking.\n",
    "  predict = predict.data.cpu().numpy()\n",
    "  #make the numpy array to refer the memory locaiton of the tensor\n",
    "  #change to numpy array -. from tensor\n",
    "  #singma_p contains the sd of the prediciton for partical feature\n",
    "  #across all samples -. for understanding the variability of the model's prediction\n",
    "  #for each deature\n",
    "  #In a machine learning context, this operation is often performed after making\n",
    "  #predictions with a model to analyze the consistency of the predictions.\n",
    "  #A lower standard deviation indicates that the model’s predictions for that\n",
    "  #feature are more consistent, while a higher standard deviation indicates\n",
    "  #greater variability.\n",
    "  Ytest = test.data.cpu().numpy()\n",
    "  sd_p = predict.std(axis=0)\n",
    "  sd_g = Ytest.std(axis=0)\n",
    "  #calculate the mean for each column (across the rows)\n",
    "  #Ytest = np.array([[1, 4],\n",
    "                  # [2, 5],\n",
    "                  # [3, 6]])\n",
    "  #print(mean_g)  # Output: [2. 5.]\n",
    "  mean_p = predict.mean(axis=0)\n",
    "  mean_g = Ytest.mean(axis=0)\n",
    "  #True if the corresponding element in sigma_g is not equal to zero.\n",
    "  #filter out the columns/features with sd = 0  to avoid divison by 0 problem\n",
    "  #in subsequent correlation function\n",
    "  #output a boolean index array with true/false\n",
    "  #index: boolean array\n",
    "  index = (sd_g != 0)\n",
    "  #calculate the correlation for each column\n",
    "  correlation = ((predict - mean_p) * (Ytest - mean_g)).mean(axis = 0)/(sd_p * sd_g)\n",
    "  #correlation for the column that has index with true in boolean index array\n",
    "  correlation = (correlation[index]).mean()\n",
    "  return rse, rae, correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1715757178832,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "6MC1Zsm4BhiI",
    "outputId": "083b17c4-82f4-407d-d2bf-11f37fea756b"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "#argparse to parse the input argument which take a path\n",
    "parser = argparse.ArgumentParser(description='Pytorch Time series forecasting')\n",
    "# parser.add_argument('--data', type = str, required=True, help='location of the data file')\n",
    "# parser.add_argument('--ntp', type=int, default=12)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "#only parse known arguments, and store unknown arguments in the unknown variable\n",
    "args, unknown = parser.parse_known_args()\n",
    "args.data = 'data/electricity.txt'\n",
    "args.window = 24 * 7\n",
    "args.hidRNN = 100\n",
    "args.hidCNN = 100\n",
    "args.hidSkip = 5\n",
    "args.CNN_kernel = 6\n",
    "args.skip = 24\n",
    "args.gpu = 1\n",
    "args.cuda = True\n",
    "args.highway_window = 24\n",
    "args.dropout = 0.2\n",
    "args.output_fun = 'sigmoid'\n",
    "args.model = 'LSTNet'\n",
    "args.batch_size = 128\n",
    "args.seed = 54321\n",
    "args.L1Loss = True\n",
    "args.optim = 'adam'\n",
    "args.lr = 0.01\n",
    "args.clip = 10\n",
    "args.epochs = 2\n",
    "args.save = '/Users/muhammadabdullahakif/Documents/GitHub/Electricity-Load-Prediction/model/model.pt' #pt: performace track\n",
    "args.horizon = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1715757179869,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "h8XaMMg36CWy",
    "outputId": "2b0b08ca-d722-40ee-d137-5ecd7f755ed6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Gate, self).__init__()\n",
    "        # Initialize weights and bias for the gate\n",
    "        self.W = nn.Parameter(torch.randn(output_size*2,output_size))\n",
    "        self.b = nn.Parameter(torch.zeros(output_size, 1))\n",
    "        # Bottleneck transformation layer [321, 128]\n",
    "        self.bottleneck = nn.Linear(input_size, output_size)\n",
    "        self.bn_concat = nn.Linear(input_size, output_size*2)\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        x_t = x_t.to(device)\n",
    "        h_prev = h_prev.to(device)\n",
    "        #print(x_t.shape)\n",
    "        # Transform x_t to the correct size using the bottleneck layer\n",
    "        x_t_transformed = self.bottleneck(x_t)\n",
    "        #print(x_t_transformed.shape)\n",
    "        # Concatenate transformed input and previous hidden state along the feature dimension\n",
    "        concat = torch.cat((x_t_transformed.t(), h_prev), dim=1)\n",
    "        #print(concat.shape)\n",
    "        # Compute the gate's output\n",
    "        #torch.matmul = dot product\n",
    "\n",
    "        z = torch.matmul(concat,self.W) + self.b\n",
    "        gate_output = torch.sigmoid(z)\n",
    "\n",
    "        return gate_output\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class ForgetGate(Gate):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__(input_size, output_size)\n",
    "\n",
    "  def forward(self, x_t, h_prev):\n",
    "    f_t = super().forward(x_t, h_prev)\n",
    "    return f_t\n",
    "\n",
    "class InputGate(Gate):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__(input_size, output_size)\n",
    "\n",
    "    #Control mu embedding\n",
    "    self.w_C = nn.Parameter(torch.randn(output_size*2, output_size))\n",
    "    self.b_C = nn.Parameter(torch.zeros(output_size, 1))\n",
    "\n",
    "  def control_forward(self, x_t, h_prev):\n",
    "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      #print(x_t.shape)\n",
    "      x_t_transformed = self.bottleneck(x_t).to(device)\n",
    "      #print(x_t_transformed.shape)\n",
    "      h_prev = h_prev.to(device)\n",
    "      concat = torch.cat((x_t_transformed, h_prev), dim=1)\n",
    "\n",
    "      # Use matmul for matrix multiplication\n",
    "      temp = torch.matmul(concat, self.w_C)\n",
    "      #print(temp.shape)\n",
    "      C_mu = torch.tanh(temp + self.b_C)\n",
    "      return C_mu\n",
    "\n",
    "\n",
    "  def forward(self,x_t, h_prev):\n",
    "    i_t = super().forward(x_t, h_prev)\n",
    "    C_t = self.control_forward(x_t, h_prev)\n",
    "    return i_t, C_t\n",
    "\n",
    "class OutputGate(Gate):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__(input_size, output_size)\n",
    "\n",
    "  def forward(self, x_t, h_prev):\n",
    "    o_t = super().forward(x_t, h_prev)\n",
    "    return o_t\n",
    "\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    # Bottleneck transformation layer\n",
    "    self.bottleneck = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "\n",
    "    #Initialisise gates\n",
    "    self.input_gate = InputGate(input_size, hidden_size)\n",
    "    self.forget_gate = ForgetGate(input_size, hidden_size)\n",
    "    self.output_gate = OutputGate(input_size, hidden_size)\n",
    "    self.cell_state = nn.Parameter(torch.zeros(hidden_size,1))\n",
    "\n",
    "  def forward(self, x_t, h_prev, c_prev):\n",
    "      # Move all tensors to the same device as the model\n",
    "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      x_t, h_prev, c_prev = x_t.to(device), h_prev.to(device), c_prev.to(device)\n",
    "\n",
    "      i_t, c_mu = self.input_gate.forward(x_t, h_prev)\n",
    "      f_t = self.forget_gate.forward(x_t, h_prev)\n",
    "      o_t = self.output_gate.forward(x_t, h_prev)\n",
    "\n",
    "      #print(\"f_t\")\n",
    "      #print(f_t.shape)\n",
    "\n",
    "      #print(f_t.shape)\n",
    "      #print(c_prev.shape)\n",
    "      c_t = f_t * c_prev + i_t * c_mu\n",
    "\n",
    "      # Compute the current hidden state\n",
    "      h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "      return h_t, c_t\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size=128):\n",
    "    # = super(LSTM,self).__init__()\n",
    "\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "    self.inver_bottleneck = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "  def forward(self, batch_size, input_sequence):\n",
    "      # Initialize hidden state and cell state for each sequence in the batch\n",
    "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      h_t = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "      c_t = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "\n",
    "      for t in range(len(input_sequence)):\n",
    "          x_t = input_sequence[:, t, :].to(device)\n",
    "          #print(x_t.shape)\n",
    "          h_t, c_t = self.lstm_cell(x_t, h_t, c_t)\n",
    "\n",
    "      return h_t, self.inver_bottleneck(c_t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1715757183513,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "c1DRZyeVAj5w",
    "outputId": "5ea4a828-bcc7-48f3-dc03-ecc2fb62ef9c"
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np;\n",
    "import importlib\n",
    "\n",
    "def train(data, X, Y, model, criterion, optim, batch_size):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  #set model to training mode\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  n_samples = 0\n",
    "  for X, Y in data.get_batches(X,Y, batch_size, True):\n",
    "    #reset the model parameters before training\n",
    "    model.zero_grad()\n",
    "    output = model(batch_size,X)\n",
    "    output = output[1]\n",
    "    #expnad the scaling matrix into the of output\n",
    "    #.size() method on the output tensor to get its size.\n",
    "    #The arguments (0, data.m) indicate that you want to expand data.scale to\n",
    "    #have the same size as the first dimension of output and the size\n",
    "    #of data.m for the second dimension.\n",
    "\n",
    "    #By expanding data.scale, you can ensure that it has the same size as output\n",
    "    #for broadcasting purposes, which is often needed in operations like element-\n",
    "    #wise multiplication or addition.\n",
    "    data.scale = data.scale\n",
    "    scale = data.scale.expand(output.shape[0], data.m)\n",
    "    #print(scale.shape)\n",
    "    #criterion: loss function measure difference between the predicted outputs and the true values.\n",
    "    Y = Y\n",
    "    #print(output.shape)\n",
    "    loss = criterion(output*scale, Y * scale)\n",
    "    loss.backward()\n",
    "    #gradient clipping to prevent gradient explosion or diminishing\n",
    "    # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    #step function to call the model to update the parameters based on the computed gradients.\n",
    "    grad_norm = optim.step()\n",
    "    total_loss += loss.item() ## Extract the loss value as a Python float\n",
    "    #number of samples = first dimension size of the output * dataset second dimension\n",
    "    #calculates the total number of elements in the current batch by multiplying the\n",
    "    #batch size by the number of features or time steps. This product is then added\n",
    "    #to the n_samples variable, which accumulates the total number of elements processed\n",
    "    #over multiple batches\n",
    "    n_samples += (output.size(0) * data.m)\n",
    "  return total_loss/n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117126,
     "status": "ok",
     "timestamp": 1715757311572,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "rSzpU6yucMPj",
    "outputId": "18761944-a29d-4eee-e285-6bb5fd9dec6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Epoch 1, Loss: 0.9896705746650696\n",
      "Epoch 2, Loss: 0.08542749285697937\n",
      "Validation: Epoch 2, Loss: 0.11343781650066376\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "##Optimasation\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "class Optim(object):\n",
    "  def _makeOptimizer(self):\n",
    "    if self.method == 'sgd':\n",
    "      self.optimizer = optim.SGD(self.params, lr=self.lr)\n",
    "    elif self.method == 'adagrad':\n",
    "      self.optimizer = optim.Adagrad(self.params, lr=self.lr)\n",
    "    elif self.method == 'adadelta':\n",
    "      self.optimizer = optim.Adadelta(self.params, lr=self.lr)\n",
    "    elif self.method == 'adam':\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.lr)\n",
    "    else:\n",
    "          raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "  def __init__(self, params, method, lr, max_grad_norm, lr_decay=1, start_decay_at=None):\n",
    "    self.params = list(params) #params may be a generator\n",
    "    self.lr = lr\n",
    "    #max_grad_norm maximum normalise gradient allowed before clipping\n",
    "    self.max_grad_norm = max_grad_norm\n",
    "    self.method = method\n",
    "    self.lr_decay = lr_decay\n",
    "    self.start_decay_at = start_decay_at\n",
    "    self.start_decay = False\n",
    "\n",
    "    self._makeOptimizer()\n",
    "\n",
    "  def step(self):\n",
    "    #computer gradients norm\n",
    "    grad_norm = 0\n",
    "    for param in self.params:\n",
    "      #etrieves the gradient data for a parameter of the neural network\n",
    "      #norm() squares the L2 norm (Euclidean norm) of the gradient tensor.\n",
    "      grad_norm += math.pow(param.grad.data.norm(),2)#since 2\n",
    "      #accumulates the sum of the squared norms of all parameters’ gradients.\n",
    "    grad_norm = math.sqrt(grad_norm)\n",
    "    #squre root to get the overal L2 norm\n",
    "\n",
    "    if grad_norm > 0:\n",
    "      shrinkage = self.max_grad_norm / grad_norm\n",
    "    else:    #1.: float-point number\n",
    "      shrinkage = 1.\n",
    "\n",
    "    for param in self.params:\n",
    "      #if meet the threshold, apply gradient cliping\n",
    "      if shrinkage < 1:\n",
    "        #apply gradient clipping for each parameter's gradient\n",
    "        param.grad.data.mul_(shrinkage)\n",
    "\n",
    "    self.optimizer.step()\n",
    "    return grad_norm\n",
    "\n",
    "  #decay learning rate if not improve on val perf\n",
    "  #or change start_decay_limit to true\n",
    "  def upadateLearningRate(self, ppl, epoch):\n",
    "      #decide which epoch to start decaying the learning rate\n",
    "    if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
    "      self.start_decay = True\n",
    "      # stores the perplexity value from the last evaluation\n",
    "      #ppl: Perplexity (PPL): It is a metric used to evaluate language models.\n",
    "      #It’s defined as the exponentiated average negative log-likelihood of a sequence. The lower the perplexity, the better the model is at predicting the sequence1.\n",
    "    if self.last_ppl is not None and ppl > self.last_ppl:\n",
    "      self.start_decay = True\n",
    "\n",
    "    if self.start_decay:\n",
    "      self.lr = self.lr * self.lr_decay\n",
    "      print(\"Decaying learning rate to %g\" % self.lr)\n",
    "\n",
    "\n",
    "    #only decay for one epoch\n",
    "    self.start_decay = False\n",
    "\n",
    "    self.last_ppl = ppl\n",
    "\n",
    "    self._makeOptimizer()\n",
    "# model = Model(args,Data)\n",
    "# optim = Optim(\n",
    "#     model.parameters(), args.optim, args.lr, args.clip,\n",
    "# )\n",
    "\n",
    "#LSTM\n",
    "model = LSTM((len(Data.rawdat[0])))\n",
    "optim = Optim(\n",
    "    model.parameters(), args.optim, args.lr, args.clip,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, data, args, train_func, evaluate_func, optim):\n",
    "  best_val = 10000000;\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  #move everything to the same device - GPU\n",
    "  if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    #moving X to GPU\n",
    "    Data.train[0] = Data.train[0].cuda()\n",
    "    Data.train[1].cuda()\n",
    "\n",
    "    if not args.cuda:\n",
    "      print(\"WARNING, have gpu, should run with --cuda\")\n",
    "    else:\n",
    "      torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "  if args.L1Loss:\n",
    "    #L1 loss = MAE (Mean Absolute Error) loss  mean of absolute difference\n",
    "    #between target value and predictions\n",
    "\n",
    "    # criterion = nn.L1Loss(size_average=False)\n",
    "    #average = loss -> losses are summed\n",
    "    criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "\n",
    "  else:\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    #set up L2 loss - MSE loss during validation or testing\n",
    "  evaluateL2 = nn.MSELoss(reduction='sum')\n",
    "  evaluateL1 = nn.L1Loss(reduction='sum')\n",
    "\n",
    "  #.cuda(): This method transfers the loss function to the GPU.\n",
    "  if args.cuda:\n",
    "    criterion = criterion.cuda()\n",
    "    evaluateL1 = evaluateL1.cuda()\n",
    "    evaluateL2 = evaluateL2.cuda()\n",
    "\n",
    "  # Define loss function and optimizer\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  # At any point you can hit Ctrl + C to break out of training early.\n",
    "  try:\n",
    "      print('begin training');\n",
    "      #'Namespace' object has no attribute 'epochs' = args.epoch is not defined\n",
    "      model = model.to(device)\n",
    "      for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        X = Data.train[0].to(device)\n",
    "        Y = Data.train[1].to(device)\n",
    "\n",
    "\n",
    "        for x, y in data.get_batches(X,Y, args.batch_size, True):\n",
    "          if len(x) < args.batch_size:\n",
    "            #overlapping batch_size, sequence length, feature_size/sensor num\n",
    "\n",
    "            #print(input_sequence.size(0)) [128, 168, 321]\n",
    "\n",
    "            padding_size =  args.batch_size - len(x)\n",
    "            # Create a tensor of zeros for padding\n",
    "\n",
    "            x_padding = torch.zeros(padding_size, x.size(1), x.size(2)).to(device)\n",
    "            y_padding = torch.zeros(padding_size, x.size(2)).to(device)\n",
    "            # print(padding.size())\n",
    "            # Concatenate the padding to the x\n",
    "            x = torch.cat((x, x_padding), dim=0)\n",
    "            # Concatenate the padding to the x\n",
    "            y = torch.cat((y, y_padding), dim=0)\n",
    "          #reset the model parameters before training\n",
    "          model.zero_grad()\n",
    "          hidden_state, output = model(args.batch_size,x)\n",
    "          loss = criterion(output, y)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "          X_val = Data.valid[0].to(device)\n",
    "          Y_val = Data.valid[1].to(device)\n",
    "          for x_val, y_val in data.get_batches(X_val,Y_val, args.batch_size, True):\n",
    "            if len(x_val) < args.batch_size:\n",
    "\n",
    "                val_padding_size =  args.batch_size - len(x_val)\n",
    "                xval_padding = torch.zeros(val_padding_size, x_val.size(1), x_val.size(2)).to(device)\n",
    "                yval_padding = torch.zeros(val_padding_size, x_val.size(2)).to(device)\n",
    "                # print(padding.size())\n",
    "                # Concatenate the padding to the x\n",
    "                x_val = torch.cat((x_val, xval_padding), dim=0)\n",
    "                # Concatenate the padding to the x\n",
    "                y_val = torch.cat((y_val, yval_padding), dim=0)\n",
    "\n",
    "            hidden_state, valid_output = model(args.batch_size,x_val)\n",
    "            val_loss = criterion(valid_output,y_val)\n",
    "\n",
    "            if val_loss.item() < best_val:\n",
    "              with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "              best_val = val_loss.item()\n",
    "          #torch.device('cuda:0') for the first GPU or torch.device('cpu') for the CPU1.\n",
    "          #then .to(device) would go to either CPU or GPU\n",
    "          #.cude(1) go to first GPU\n",
    "          # model.to('cuda')\n",
    "\n",
    "          print(f\"Validation: Epoch {epoch}, Loss: {val_loss.item()}\")\n",
    "          if val_loss.item() < best_val:\n",
    "            with open(args.save, 'wb') as f:\n",
    "              torch.save(model, f)\n",
    "            best_val = val_loss.item()\n",
    "        # if epoch % 5 == 0:\n",
    "        #   #torch.device('cuda:0') for the first GPU or torch.device('cpu') for the CPU1.\n",
    "        #   #then .to(device) would go to either CPU or GPU\n",
    "        #   #.cude(1) go to first GPU\n",
    "        #   # model.to('cuda')\n",
    "        #   test_acc, test_rae, test_corr  = evaluate(Data, x_test, y_test, model, evaluateL2, evaluateL1, args.batch_size);\n",
    "        #   print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "\n",
    "  except KeyboardInterrupt:\n",
    "      print('-' * 89)\n",
    "      print('Exiting from training early')\n",
    "\n",
    "\n",
    "  # test_acc, test_rae, test_corr  = evaluate(Data, x_test, y_test, model, evaluateL2, evaluateL1, args.batch_size);\n",
    "  # print (\"test rse {:5.4f} | test rae {:5.4f} | test corr {:5.4f}\".format(test_acc, test_rae, test_corr))\n",
    "\n",
    "\n",
    "train_and_evaluate(model, Data, args, train, evaluate, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1715757880891,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "WFK5e5i2DMjj",
    "outputId": "41886eba-b5c1-4f03-ed41-c28358145d5d"
   },
   "outputs": [],
   "source": [
    "def model_pred(data,X,Y):\n",
    "  try:\n",
    "    for x, y in data.get_batches(X,Y, args.batch_size, True):\n",
    "      with torch.no_grad():\n",
    "        if len(x) < args.batch_size:\n",
    "          #overlapping batch_size, sequence length, feature_size/sensor num\n",
    "\n",
    "          #print(input_sequence.size(0)) [128, 168, 321]\n",
    "\n",
    "          padding_size =  args.batch_size - len(x)\n",
    "          # Create a tensor of zeros for padding\n",
    "          device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "          x_padding = torch.zeros(padding_size, x.size(1), x.size(2)).to(device)\n",
    "          y_padding = torch.zeros(padding_size, x.size(2)).to(device)\n",
    "          # print(padding.size())\n",
    "          # Concatenate the padding to the x\n",
    "          x = torch.cat((x, x_padding), dim=0)\n",
    "          # Concatenate the padding to the x\n",
    "          y = torch.cat((y, y_padding), dim=0)\n",
    "          hidden_state, output = model(args.batch_size,x)\n",
    "          #print(output)\n",
    "          return hidden_state, output\n",
    "  except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1073,
     "status": "ok",
     "timestamp": 1715758267637,
     "user": {
      "displayName": "Dizhen Liang",
      "userId": "06557023756950998584"
     },
     "user_tz": -480
    },
    "id": "jmHm19r9CrzT",
    "outputId": "aa27b081-d091-46f6-802c-0f3fcc94e808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01768552  0.5279969  -0.05627704 ...  0.18000312 -0.0092612\n",
      "   0.5488242 ]\n",
      " [ 0.13710977  0.8422408   0.23776245 ...  0.44833946  0.7772954\n",
      "   0.20016316]\n",
      " [-0.2687753   0.35739878 -0.20909263 ... -0.16483785  0.66273916\n",
      "  -0.59740585]\n",
      " ...\n",
      " [ 0.03493487 -0.19204521  0.06215587 ...  0.16945633 -0.06411807\n",
      "  -0.08294916]\n",
      " [-1.028922   -0.08594657 -0.02215187 ...  0.7626117   0.04881186\n",
      "  -0.5249645 ]\n",
      " [-0.6680805   0.26595524 -0.7043383  ... -0.26113236 -0.30365384\n",
      "   0.4340835 ]]\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n",
      "-0.017685516\n",
      "0.5279969\n",
      "-0.056277037\n",
      "0.15943415\n",
      "0.16665395\n",
      "0.14395541\n",
      "0.2091797\n",
      "0.18000312\n",
      "-0.009261204\n",
      "0.5488242\n"
     ]
    }
   ],
   "source": [
    "  # Load the best saved model.\n",
    "#output = None\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X = Data.test[0].to(device)\n",
    "    Y = Data.test[1].to(device)\n",
    "\n",
    "    hidden_state, output = model_pred(Data,X,Y)\n",
    "    output = output.cpu()\n",
    "    output = output.numpy()\n",
    "    print(output)\n",
    "\n",
    "    for i in range(len(output)):\n",
    "      for j in range(len(output[0])):\n",
    "        print(output[0][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(training_file_path):\n",
    "    # Define your arguments or use argparse to pass them\n",
    "    args.data = training_file_path\n",
    "    data = Data_util(args.data, 0.6, 0.2, args.cuda, args.horizon, args.window, 2)\n",
    "    model = LSTM((len(data.rawdat[0])))\n",
    "    optim = Optim(model.parameters(), args.optim, args.lr, args.clip)\n",
    "    train_and_evaluate(model, data, args, train, evaluate, optim)\n",
    "\n",
    "# If you want to call this script from the command line, uncomment the following:\n",
    "# if __name__ == '__main__':\n",
    "#     import sys\n",
    "#     main(sys.argv[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
